{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iueVqaZCRx6O"
      },
      "source": [
        "### **Due Date**\n",
        "9/5/2024 at 11:59PM EST\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Welcome to Assignment 0 of 4756. In this short notebook, we hope to refresh your memory with common Numpy pain points and PyTorch. Please read through the following paragraphs carefully, as they will apply to this and all future assignments.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024fa/#resources) section on the course website is your friend (especially for this first assignment)! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. Of course, Numpy and Python skills will also be implicitly tested via this assignment. If you need a refresher, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) for PyTorch! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run this notebook, you can see [this guide](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) for VSCode. Create a new virtual environment and then activate it to install the requirements. If the virtual environment you created through VS Code is called `.venv`, you can run\n",
        "```bash\n",
        "source .venv/bin/activate\n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x110fb6a30>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Reload code in q1 and q2 modules without having to restart the kernel\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: Shapes and Sizes\n",
        "\n",
        "In this first question, we hope to get you familiar thinking about and working with shapes in both Numpy and PyTorch. The most common pain points we see in this class are related to debugging shape and sizing issues. To help you get familiar with this, we've provided some buggy code that needs to be debugged.\n",
        "\n",
        "There are 3 functions to modify in this section, located in `q1.py`. Below, `calculate_distances` is called with some inputs and there are two assert statements that check for the correct shape and correct numbers. Please modify `calculate_distances` in `q1.py` to pass the asserts. You should not need to modify the inputs. The bugs are also simple enough to be fixed in-line - you should not need to add many lines of code.\n",
        "\n",
        "To debug, we suggest printing out each intermediate step of your code to check that the shapes are what you expect them to be. If you are confused about the output of a print statement, check out the documentation or ask for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Wrong shape: () != (3, 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      7\u001b[0m distance \u001b[38;5;241m=\u001b[39m q1\u001b[38;5;241m.\u001b[39mcalculate_distances(x, y)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m distance\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistance\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(n,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(distance, np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m4\u001b[39m]])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Wrong shape: () != (3, 1)"
          ]
        }
      ],
      "source": [
        "import q1\n",
        "\n",
        "x = np.array([[0, 0], [1, 0], [2, 0]])\n",
        "y = np.array([[3, 4], [3, 0], [-2, 0]])\n",
        "n, d = x.shape\n",
        "\n",
        "distance = q1.calculate_distances(x, y)\n",
        "assert distance.shape == (n, 1), f\"Wrong shape: {distance.shape} != {(n, 1)}\"\n",
        "assert np.array_equal(distance, np.array([[5], [2], [4]])), f\"Wrong values: {distance}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once you have fixed `calculate_distances` and the assert statements are passing, try fixing function `combine_squares` below. Now you are working with PyTorch tensors but they are very similar to Numpy arrays. Use the same debugging principles as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LR_MrK8aXQ6U"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Wrong shape: torch.Size([8, 2]) != (4, 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m n, _ \u001b[38;5;241m=\u001b[39m square_1\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      7\u001b[0m combined_square \u001b[38;5;241m=\u001b[39m q1\u001b[38;5;241m.\u001b[39mcombine_squares(square_1, square_2, square_3, square_4)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m combined_square\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_square\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mn,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(combined_square, torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m], [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m ,\u001b[38;5;241m11\u001b[39m], [\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m]])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_square\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Wrong shape: torch.Size([8, 2]) != (4, 4)"
          ]
        }
      ],
      "source": [
        "square_1 = torch.tensor([[0 , 1 ], [4 , 5 ]])\n",
        "square_2 = torch.tensor([[2 , 3 ], [6 , 7 ]])\n",
        "square_3 = torch.tensor([[8 , 9 ], [12, 13]])\n",
        "square_4 = torch.tensor([[10, 11], [14, 15]])\n",
        "n, _ = square_1.shape\n",
        "\n",
        "combined_square = q1.combine_squares(square_1, square_2, square_3, square_4)\n",
        "assert combined_square.shape == (2 * n, 2 * n), f\"Wrong shape: {combined_square.shape} != {(2 * n, 2 * n)}\"\n",
        "assert torch.equal(combined_square, torch.tensor([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10 ,11], [12, 13, 14, 15]])), f\"Wrong values: {combined_square}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you will combine your knowledge from the previous two functions and implement `video_to_filmstrip`. We omit the assert statements here so you should write your own to verify the correctness of your code. For reference, the solution is 3 lines of code. Your code should be vectorized meaning do not use any for loops and only use Numpy/PyTorch functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     video_frames\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1080\u001b[39m, \u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m*\u001b[39m i)\n\u001b[0;32m----> 5\u001b[0m filmstrip \u001b[38;5;241m=\u001b[39m \u001b[43mq1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_to_filmstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/cs4756-robot-learning-fa24/assignments/A0/q1.py:73\u001b[0m, in \u001b[0;36mvideo_to_filmstrip\u001b[0;34m(video_frames)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert video frames into a filmstrip image.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mThis function takes in a list of numpy arrays where each array\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        A tensor with shape (h, w * n_frames, c)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# TODO: Implement\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "video_frames = []\n",
        "for i in range(10):\n",
        "    video_frames.append(np.ones((1080, 1920, 3)) * i)\n",
        "\n",
        "filmstrip = q1.video_to_filmstrip(video_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLvDDDL6V8MP"
      },
      "source": [
        "### Q2: A Simple Regressor\n",
        "\n",
        "This second question will hopefully refresh your memory of how to train a model in PyTorch and give you a general idea of what coding assignments will look like in this class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwBTSbGPXEIM"
      },
      "source": [
        "In this question, we are testing your ability to solve a simple regression problem. First we generate some train and test data. This should not be modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-q3zMdvpYBAS"
      },
      "outputs": [],
      "source": [
        "# Generate train/test data\n",
        "data_size = 500\n",
        "X = np.random.rand(data_size, 1)\n",
        "Y = -4 * X + 2 + np.random.randn(data_size, 1)\n",
        "\n",
        "test_split = int(data_size * 0.8)\n",
        "x_train, x_test = X[:test_split], X[test_split:]\n",
        "y_train, y_test = Y[:test_split], Y[test_split:]\n",
        "\n",
        "x_train, y_train = torch.tensor(x_train).float(), torch.tensor(y_train).float()\n",
        "x_test, y_test = torch.tensor(x_test).float(), torch.tensor(y_test).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now your task is to implement a simple linear regressor. Refer to script `q2.py` and implement the TODOs. You will need to implement the `forward()` pass, specify a loss function and optimizer, and write a training loop that includes checkpointing logic to save the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2rwUxK-ZG-i"
      },
      "outputs": [],
      "source": [
        "import q2\n",
        "checkpoint_path = q2.get_checkpoint_path()\n",
        "model = q2.LinearRegression()\n",
        "loss_fn, optimizer = q2.create_loss_and_optimizer(model)\n",
        "q2.train(x_train, y_train, model, loss_fn, optimizer, checkpoint_path, num_epochs=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should also implement `load_model_checkpoint` so you can load your best model to evaluate. This is useful for us to grade your assignment. For future assignments, you can also save information such as the optimizer state and epochs to resume training if your training is interrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11U8PwA4Z-XJ"
      },
      "outputs": [],
      "source": [
        "model = q2.load_model_checkpoint(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnQUGOpnjXgH"
      },
      "source": [
        "Below we provide code snippets to visualize the performance of your model on the training and test data. You do not need to modify this code - use this as a sanity check to make sure your training code is working. The test plot will be saved to `q2_test.png` - this must be included in your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhqeSuoir9u"
      },
      "outputs": [],
      "source": [
        "# Plot training performance \n",
        "plt.scatter(x_train, y_train, s=10)\n",
        "with torch.no_grad():\n",
        "  y_pred = model(x_train)\n",
        "y_pred = y_pred.numpy()\n",
        "plt.plot(x_train, y_pred, color='magenta')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEzgKkiYj1uT"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "  y_pred = model(x_test)\n",
        "  test_loss = loss_fn(y_pred, y_test)\n",
        "  print(f\"Test loss: {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlCE4DmGk-iZ"
      },
      "outputs": [],
      "source": [
        "# Plot test performance \n",
        "plt.scatter(x_test, y_test, s=10)\n",
        "with torch.no_grad():\n",
        "  y_pred = model(x_test)\n",
        "y_pred = y_pred.numpy()\n",
        "plt.plot(x_test, y_pred, color='red')\n",
        "plt.savefig('q2_test.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
